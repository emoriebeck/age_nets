[
["index.html", "Interindividual Age Differences in Personality Structure Chapter 1 Introduction", " Interindividual Age Differences in Personality Structure Emorie D Beck David M Condon Joshua J Jackson 2019-07-09 Chapter 1 Introduction The following site contains the code to conduct the analyses in Beck, Condon, &amp; Jackson (2019), whic his available on Psyarxiv. All data and models are available as .rdata files on github, so the results can be reproduced without downloading the full directory or data. Here is the abstract: Most investigations in the structure of personality traits do not adequately address age; instead they presuppose a constant structure across the lifespan. Further, few studies look at the structure of personality traits a-theoretically, often neglecting to examine the relationship among indicators within a trait (coherence) and across traits (differentiation). Using a network approach, the present study examines (1) age differences in differentiation and coherence, (2) the similarity between the Big Five and network structures, and (3) the consistency of network structure across age groups in a large, cross-sectional sample. Results indicate that coherence shows early gains in adolescence with few changes across the lifespan, while differentiation mostly weakens across adulthood. The result of these age-related changes is that Big Five indicators only parallel the Big Five structure among young adults, but not older adults. The structure of young adults tends to be quite similar while the network structures of older adults appear to greatly differ from one another. These results suggest that older adults have a different structure of personality than younger adults and suggest that future research should not assume consistency in personality structure across the lifespan. Please address questions to edbeck@wustl.edu. "],
["background.html", "Chapter 2 Background on the SAPA Project 2.1 Packages 2.2 Methods", " Chapter 2 Background on the SAPA Project 2.1 Packages library(qgraph) library(bootnet) library(ggplot2) library(psych) library(RColorBrewer) library(knitr) library(kableExtra) library(parallel) library(broom) library(igraph) library(gridExtra) library(data.table) library(plyr) library(tidyverse) 2.2 Methods All of the data being pulled in have previously been cleaned. There are two big files â€” one from 2006 to 2010 and a second from 2010 to now. 2.2.1 2006 to 2010 The older data set is based on the predecessor platform to SAPA (a 50-item survey, created by William Revelle and hosted at www.personalityproject.org). There are a couple of issues with this sample. First, there was much less demographic information collected at that time (age, gender, ethnicity, education, and a few others). Second, it only sampled 50 items from the IPIP100 (each participant got 2 of 4 25-item blocks) and a preliminary set of ICAR items, most of which have since been deprecated. Third, there were two technical problems that caused a complete lack of data collection for one of the IPIP items (q_55) and a partial lack of data collection for two others (q_316 and q_995). Still, these data remain quite useful in that they include responses on 99 of the 100 IPIP items from about 114k participants. The following code chunk pulls in the first sample and describes it. Note that only 99 variables are listed because item q_55 is empty. data_path &lt;- &quot;https://github.com/emoriebeck/age_nets/blob/master&quot; load(url(sprintf(&quot;%s/data/SAPAdata4apr2006to18aug2010.rdata?raw=true&quot;, data_path))) dim(taFinal) # Participants who did not answer any IPIP100 items removed. ## [1] 114016 141 # Identify Ps who didnt answer any IPIP100 items empty &lt;- rowSums(subset(taFinal, select = c(q_76:q_1989)), na.rm = TRUE) taFinal &lt;- taFinal[empty &gt; 0, ] dim(taFinal) # Participants who did not answer any IPIP100 items removed. ## [1] 113961 141 describe(subset(taFinal, select = c(q_76:q_1989)), fast = TRUE) ## vars n mean sd min max range se ## q_76 1 56860 4.20 1.27 1 6 5 0.01 ## q_108 2 56739 3.29 1.45 1 6 5 0.01 ## q_124 3 56014 4.48 1.22 1 6 5 0.01 ## q_128 4 55964 4.81 1.13 1 6 5 0.00 ## q_132 5 57010 4.77 1.09 1 6 5 0.00 ## q_140 6 56719 3.41 1.65 1 6 5 0.01 ## q_146 7 56131 2.30 1.34 1 6 5 0.01 ## q_150 8 56781 4.89 1.16 1 6 5 0.00 ## q_177 9 56719 3.41 1.49 1 6 5 0.01 ## q_194 10 56665 2.47 1.33 1 6 5 0.01 ## q_195 11 56813 2.52 1.36 1 6 5 0.01 ## q_200 12 56783 2.29 1.29 1 6 5 0.01 ## q_217 13 57159 4.73 1.22 1 6 5 0.01 ## q_240 14 56685 4.86 1.04 1 6 5 0.00 ## q_241 15 56739 3.95 1.57 1 6 5 0.01 ## q_248 16 56715 4.05 1.34 1 6 5 0.01 ## q_254 17 57135 4.33 1.37 1 6 5 0.01 ## q_262 18 56926 3.28 1.51 1 6 5 0.01 ## q_316 19 5723 2.71 1.54 1 6 5 0.02 ## q_403 20 57115 3.83 1.54 1 6 5 0.01 ## q_422 21 57012 4.66 1.18 1 6 5 0.00 ## q_492 22 55996 4.42 1.19 1 6 5 0.01 ## q_493 23 57076 4.97 1.05 1 6 5 0.00 ## q_497 24 56707 3.52 1.53 1 6 5 0.01 ## q_530 25 56043 4.27 1.30 1 6 5 0.01 ## q_609 26 56727 2.14 1.33 1 6 5 0.01 ## q_619 27 56025 4.30 1.27 1 6 5 0.01 ## q_626 28 56009 2.65 1.38 1 6 5 0.01 ## q_690 29 56718 3.84 1.48 1 6 5 0.01 ## q_698 30 56739 3.74 1.62 1 6 5 0.01 ## q_712 31 56031 2.94 1.60 1 6 5 0.01 ## q_815 32 57031 4.33 1.32 1 6 5 0.01 ## q_819 33 56746 4.35 1.35 1 6 5 0.01 ## q_838 34 56689 2.18 1.35 1 6 5 0.01 ## q_844 35 56669 4.68 1.22 1 6 5 0.01 ## q_890 36 57094 2.89 1.49 1 6 5 0.01 ## q_901 37 56065 3.24 1.56 1 6 5 0.01 ## q_904 38 57159 3.23 1.55 1 6 5 0.01 ## q_931 39 56644 3.92 1.48 1 6 5 0.01 ## q_952 40 56020 2.92 1.52 1 6 5 0.01 ## q_960 41 57004 3.71 1.42 1 6 5 0.01 ## q_962 42 56794 3.34 1.54 1 6 5 0.01 ## q_974 43 56025 3.50 1.50 1 6 5 0.01 ## q_979 44 57034 3.64 1.59 1 6 5 0.01 ## q_986 45 56862 3.73 1.56 1 6 5 0.01 ## q_995 46 5795 3.27 1.53 1 6 5 0.02 ## q_1020 47 56943 3.51 1.39 1 6 5 0.01 ## q_1041 48 57064 4.23 1.28 1 6 5 0.01 ## q_1050 49 56791 4.36 1.30 1 6 5 0.01 ## q_1053 50 56750 4.71 1.27 1 6 5 0.01 ## q_1058 51 56700 4.84 1.20 1 6 5 0.01 ## q_1083 52 56637 1.97 1.15 1 6 5 0.00 ## q_1088 53 56773 2.49 1.32 1 6 5 0.01 ## q_1090 54 56684 4.74 1.00 1 6 5 0.00 ## q_1099 55 56135 3.23 1.58 1 6 5 0.01 ## q_1114 56 56753 2.61 1.46 1 6 5 0.01 ## q_1162 57 55998 4.83 1.13 1 6 5 0.00 ## q_1163 58 56879 2.45 1.44 1 6 5 0.01 ## q_1180 59 56738 3.35 1.45 1 6 5 0.01 ## q_1205 60 56018 3.99 1.32 1 6 5 0.01 ## q_1206 61 55916 4.59 1.28 1 6 5 0.01 ## q_1254 62 57066 3.76 1.76 1 6 5 0.01 ## q_1255 63 56782 3.37 1.67 1 6 5 0.01 ## q_1290 64 56684 4.43 1.33 1 6 5 0.01 ## q_1333 65 57010 4.08 1.50 1 6 5 0.01 ## q_1364 66 56038 4.67 1.45 1 6 5 0.01 ## q_1374 67 57062 4.16 1.38 1 6 5 0.01 ## q_1385 68 57084 4.95 1.09 1 6 5 0.00 ## q_1388 69 57056 4.01 1.60 1 6 5 0.01 ## q_1392 70 57003 4.58 1.20 1 6 5 0.01 ## q_1397 71 56715 2.73 1.41 1 6 5 0.01 ## q_1410 72 56209 4.39 1.44 1 6 5 0.01 ## q_1419 73 56077 4.61 1.18 1 6 5 0.00 ## q_1422 74 57075 4.21 1.31 1 6 5 0.01 ## q_1452 75 56685 2.28 1.31 1 6 5 0.01 ## q_1479 76 55972 3.15 1.53 1 6 5 0.01 ## q_1480 77 56834 3.06 1.50 1 6 5 0.01 ## q_1483 78 56770 3.27 1.67 1 6 5 0.01 ## q_1505 79 56005 3.00 1.59 1 6 5 0.01 ## q_1507 80 56784 4.74 1.16 1 6 5 0.00 ## q_1585 81 56738 3.24 1.45 1 6 5 0.01 ## q_1677 82 56695 3.36 1.49 1 6 5 0.01 ## q_1683 83 56629 3.61 1.52 1 6 5 0.01 ## q_1696 84 56533 2.61 1.34 1 6 5 0.01 ## q_1705 85 57010 4.99 0.98 1 6 5 0.00 ## q_1738 86 56067 4.90 1.14 1 6 5 0.00 ## q_1742 87 56672 4.27 1.39 1 6 5 0.01 ## q_1763 88 56749 4.85 1.15 1 6 5 0.00 ## q_1768 89 56005 4.33 1.31 1 6 5 0.01 ## q_1775 90 57002 3.32 1.48 1 6 5 0.01 ## q_1792 91 56656 4.65 1.12 1 6 5 0.00 ## q_1803 92 56724 3.65 1.68 1 6 5 0.01 ## q_1832 93 57047 4.30 1.25 1 6 5 0.01 ## q_1861 94 56664 2.66 1.42 1 6 5 0.01 ## q_1893 95 56675 3.64 1.49 1 6 5 0.01 ## q_1913 96 57047 3.09 1.40 1 6 5 0.01 ## q_1949 97 56037 3.44 1.56 1 6 5 0.01 ## q_1964 98 55945 2.51 1.29 1 6 5 0.01 ## q_1989 99 56748 4.36 1.40 1 6 5 0.01 # table by gender table(taFinal$gender) ## ## Male Female ## 37864 76097 # describe the age distribution describe(taFinal$age, fast = TRUE) ## vars n mean sd min max range se ## X1 1 113961 27.3 11.17 14 90 76 0.03 # how many countries are represented? length(table(taFinal$country)) ## [1] 204 # show the top 25 countries sort(table(taFinal$country), decreasing = TRUE)[1:25] ## ## USA CAN GBR AUS IND MYS PHL CHN DEU SGP SWE MEX ## 83110 5661 4025 3647 1795 844 756 645 628 617 574 556 ## POL NLD IRL NZL ZAF KOR HKG BRA NOR FRA ROU PAK ## 473 453 417 398 398 352 343 317 314 304 301 235 ## ITA ## 215 # show the US states, in order sort(table(taFinal$state), decreasing = TRUE)[1:51] ## ## California Illinois New York ## 9709 5520 4942 ## Pennsylvania Texas Ohio ## 4758 4662 3600 ## Florida Virginia Michigan ## 2936 2787 2549 ## New Jersey Georgia Wisconsin ## 2495 2414 2377 ## Minnesota Louisiana Massachusetts ## 2104 2030 1935 ## Maryland Washington Indiana ## 1772 1742 1707 ## Missouri North Carolina Oregon ## 1611 1454 1203 ## New Mexico Tennessee Colorado ## 1199 1133 1097 ## South Carolina Connecticut Iowa ## 1010 986 982 ## Arizona Kentucky Kansas ## 866 820 808 ## Oklahoma Alabama Mississippi ## 771 643 604 ## Delaware Nebraska Arkansas ## 592 580 577 ## Alaska Utah Rhode Island ## 555 487 422 ## New Hampshire West Virginia Maine ## 389 384 356 ## Idaho Hawaii Nevada ## 340 292 274 ## Montana North Dakota District of Columbia ## 243 190 174 ## South Dakota Vermont Wyoming ## 172 161 126 # table by ethnicity table(taFinal$ethnic) ## &lt; table of extent 0 &gt; # table by education table(taFinal$education) ## ## Less than 12 years High school graduate ## 15052 9037 ## Some college did not graduate Currently attending college ## 10736 42080 ## College graduate Graduate or professional degree ## 18296 18760 2.2.2 2010 to 2017} From August 18, 2010 to February 7, 2017, the data were collected from the SAPA-Project.org. While many more variables (including many more demographic variables) were collected in this sample, the data are pared down to match the older set. It should also be noted that large subsets of these data have previously been placed in the public domain (see https://dataverse.harvard.edu/dataverse/SAPA-Project). Here, we pull in the second sample, subset it, and describe the result. load(url(sprintf(&quot;%s/data/SAPAdata18aug2010thru7feb2017.rdata?raw=true&quot;, data_path))) dim(SAPAdata18aug2010thru7feb2017) # Participants who did not answer any IPIP100 items removed. ## [1] 255348 953 # Identify Ps who didnt answer any IPIP100 items empty &lt;- rowSums(subset(SAPAdata18aug2010thru7feb2017, select = c(q_55:q_1989)), na.rm = TRUE) SAPAdata18aug2010thru7feb2017 &lt;- SAPAdata18aug2010thru7feb2017[empty &gt; 0, ] dim(SAPAdata18aug2010thru7feb2017) # Participants who did not answer any IPIP100 items removed. ## [1] 255190 953 SAPAdata18aug2010thru7feb2017 &lt;- SAPAdata18aug2010thru7feb2017 %&gt;% select(one_of(c(&quot;q_55&quot;, colnames(taFinal)))) %&gt;% select(RID:education, q_55, everything()) describe(subset(SAPAdata18aug2010thru7feb2017, select = c(q_55:q_1989)), fast = TRUE) ## vars n mean sd min max range se ## q_55 1 59248 4.05 1.49 1 6 5 0.01 ## q_76 2 59282 4.18 1.26 1 6 5 0.01 ## q_108 3 57880 3.21 1.45 1 6 5 0.01 ## q_124 4 70237 4.45 1.18 1 6 5 0.00 ## q_128 5 69812 4.77 1.14 1 6 5 0.00 ## q_132 6 69751 4.64 1.15 1 6 5 0.00 ## q_140 7 59834 3.41 1.63 1 6 5 0.01 ## q_146 8 68259 2.38 1.36 1 6 5 0.01 ## q_150 9 68308 4.81 1.19 1 6 5 0.00 ## q_177 10 58340 3.44 1.49 1 6 5 0.01 ## q_194 11 59219 2.43 1.31 1 6 5 0.01 ## q_195 12 68069 2.61 1.38 1 6 5 0.01 ## q_200 13 68525 2.41 1.33 1 6 5 0.01 ## q_217 14 59074 4.70 1.21 1 6 5 0.00 ## q_240 15 69644 4.82 1.05 1 6 5 0.00 ## q_241 16 59522 4.01 1.58 1 6 5 0.01 ## q_248 17 58209 4.00 1.37 1 6 5 0.01 ## q_254 18 60182 4.21 1.41 1 6 5 0.01 ## q_262 19 59494 3.18 1.54 1 6 5 0.01 ## q_316 20 69979 2.70 1.54 1 6 5 0.01 ## q_403 21 59382 3.98 1.53 1 6 5 0.01 ## q_422 22 69499 4.61 1.21 1 6 5 0.00 ## q_492 23 69740 4.41 1.20 1 6 5 0.00 ## q_493 24 69509 4.91 1.06 1 6 5 0.00 ## q_497 25 58027 3.44 1.52 1 6 5 0.01 ## q_530 26 70916 4.31 1.27 1 6 5 0.00 ## q_609 27 59701 2.19 1.36 1 6 5 0.01 ## q_619 28 70528 4.31 1.25 1 6 5 0.00 ## q_626 29 59498 2.59 1.35 1 6 5 0.01 ## q_690 30 59601 3.96 1.48 1 6 5 0.01 ## q_698 31 70353 3.54 1.61 1 6 5 0.01 ## q_712 32 59897 3.05 1.61 1 6 5 0.01 ## q_815 33 59519 4.20 1.36 1 6 5 0.01 ## q_819 34 59304 4.27 1.39 1 6 5 0.01 ## q_838 35 68245 2.35 1.46 1 6 5 0.01 ## q_844 36 68357 4.68 1.23 1 6 5 0.00 ## q_890 37 58185 2.89 1.47 1 6 5 0.01 ## q_901 38 59862 3.27 1.58 1 6 5 0.01 ## q_904 39 59520 3.21 1.54 1 6 5 0.01 ## q_931 40 70518 4.02 1.45 1 6 5 0.01 ## q_952 41 58427 2.92 1.52 1 6 5 0.01 ## q_960 42 58032 3.70 1.44 1 6 5 0.01 ## q_962 43 59196 3.37 1.54 1 6 5 0.01 ## q_974 44 58193 3.49 1.49 1 6 5 0.01 ## q_979 45 58197 3.62 1.59 1 6 5 0.01 ## q_986 46 58344 3.79 1.57 1 6 5 0.01 ## q_995 47 58004 3.25 1.52 1 6 5 0.01 ## q_1020 48 58039 3.42 1.39 1 6 5 0.01 ## q_1041 49 68142 4.30 1.26 1 6 5 0.00 ## q_1050 50 69752 4.34 1.29 1 6 5 0.00 ## q_1053 51 68453 4.70 1.26 1 6 5 0.00 ## q_1058 52 59526 4.81 1.21 1 6 5 0.00 ## q_1083 53 59048 2.00 1.17 1 6 5 0.00 ## q_1088 54 69366 2.48 1.30 1 6 5 0.00 ## q_1090 55 69251 4.67 1.03 1 6 5 0.00 ## q_1099 56 58276 3.19 1.56 1 6 5 0.01 ## q_1114 57 59641 2.72 1.47 1 6 5 0.01 ## q_1162 58 68437 4.79 1.16 1 6 5 0.00 ## q_1163 59 58733 2.40 1.41 1 6 5 0.01 ## q_1180 60 59365 3.47 1.47 1 6 5 0.01 ## q_1205 61 69960 3.92 1.34 1 6 5 0.01 ## q_1206 62 68167 4.57 1.28 1 6 5 0.00 ## q_1254 63 59382 3.71 1.73 1 6 5 0.01 ## q_1255 64 59350 3.33 1.64 1 6 5 0.01 ## q_1290 65 70597 4.49 1.27 1 6 5 0.00 ## q_1333 66 69656 4.15 1.46 1 6 5 0.01 ## q_1364 67 68534 4.63 1.47 1 6 5 0.01 ## q_1374 68 70631 4.25 1.33 1 6 5 0.01 ## q_1385 69 68461 5.01 1.09 1 6 5 0.00 ## q_1388 70 69687 3.98 1.58 1 6 5 0.01 ## q_1392 71 69585 4.58 1.20 1 6 5 0.00 ## q_1397 72 59299 2.82 1.42 1 6 5 0.01 ## q_1410 73 59784 4.30 1.47 1 6 5 0.01 ## q_1419 74 68562 4.61 1.16 1 6 5 0.00 ## q_1422 75 59488 4.27 1.29 1 6 5 0.01 ## q_1452 76 59530 2.31 1.32 1 6 5 0.01 ## q_1479 77 58257 3.17 1.53 1 6 5 0.01 ## q_1480 78 59220 3.11 1.51 1 6 5 0.01 ## q_1483 79 59564 3.29 1.64 1 6 5 0.01 ## q_1505 80 58229 3.05 1.60 1 6 5 0.01 ## q_1507 81 70717 4.80 1.13 1 6 5 0.00 ## q_1585 82 58116 3.23 1.44 1 6 5 0.01 ## q_1677 83 58010 3.27 1.49 1 6 5 0.01 ## q_1683 84 58091 3.62 1.50 1 6 5 0.01 ## q_1696 85 58914 2.60 1.34 1 6 5 0.01 ## q_1705 86 68148 4.99 1.01 1 6 5 0.00 ## q_1738 87 59777 4.93 1.13 1 6 5 0.00 ## q_1742 88 59128 4.15 1.43 1 6 5 0.01 ## q_1763 89 68336 4.83 1.18 1 6 5 0.00 ## q_1768 90 70279 4.33 1.31 1 6 5 0.00 ## q_1775 91 58307 3.19 1.48 1 6 5 0.01 ## q_1792 92 68363 4.68 1.14 1 6 5 0.00 ## q_1803 93 59615 3.55 1.70 1 6 5 0.01 ## q_1832 94 68502 4.43 1.25 1 6 5 0.00 ## q_1861 95 59797 2.72 1.43 1 6 5 0.01 ## q_1893 96 59820 3.73 1.46 1 6 5 0.01 ## q_1913 97 70046 3.08 1.41 1 6 5 0.01 ## q_1949 98 59504 3.39 1.57 1 6 5 0.01 ## q_1964 99 59480 2.46 1.25 1 6 5 0.01 ## q_1989 100 58160 4.43 1.40 1 6 5 0.01 # table by gender table(SAPAdata18aug2010thru7feb2017$gender) ## ## male female ## 95169 160020 # describe the age distribution describe(SAPAdata18aug2010thru7feb2017$age, fast = TRUE) ## vars n mean sd min max range se ## X1 1 255190 25.94 10.73 14 90 76 0.02 # how many countries are represented? length(table(SAPAdata18aug2010thru7feb2017$country)) ## [1] 220 # show the top 25 countries sort(table(SAPAdata18aug2010thru7feb2017$country), decreasing = TRUE)[1:25] ## ## USA CAN GBR AUS IND SWE DEU PHL MYS CHN ## 168816 10581 7214 4790 2782 2425 2346 2239 2151 1441 ## NLD SGP NOR NZL MEX FRA BRA ZAF FIN IRL ## 1301 1269 1170 1033 984 844 830 793 772 673 ## IDN POL ROU HKG ITA ## 646 641 623 576 568 # show the US states, in order state &lt;- subset(SAPAdata18aug2010thru7feb2017, country == &quot;USA&quot;) sort(table(state$state), decreasing = TRUE)[1:51] ## ## California Illinois Texas ## 18062 11611 10367 ## New York Florida Pennsylvania ## 8882 8054 7919 ## Michigan Ohio Virginia ## 7660 6213 5763 ## Georgia Washington Minnesota ## 5308 4753 4090 ## North Carolina Wisconsin New Jersey ## 3607 3394 3290 ## Massachusetts Indiana Maryland ## 3147 2938 2689 ## Missouri Arizona Louisiana ## 2567 2495 2487 ## South Carolina New Mexico Oregon ## 2479 2223 2213 ## Tennessee Delaware Alabama ## 2135 1929 1865 ## Colorado Kentucky Oklahoma ## 1816 1757 1643 ## Iowa Connecticut Nebraska ## 1441 1355 1348 ## Utah Kansas Mississippi ## 1309 1165 978 ## Arkansas Rhode Island Nevada ## 925 825 763 ## Idaho New Hampshire Alaska ## 683 611 578 ## Maine Montana West Virginia ## 576 553 512 ## Hawaii South Dakota Vermont ## 487 401 312 ## District of Columbia North Dakota Wyoming ## 309 268 216 # table by ethnicity table(SAPAdata18aug2010thru7feb2017$ethnic) ## &lt; table of extent 0 &gt; # table by education table(SAPAdata18aug2010thru7feb2017$education) ## ## less12yrs HSgrad CurrentInUniv SomeCollege ## 33011 17517 104783 13006 ## CollegeDegree InGradOrProSchool GradOrProDegree ## 32020 11431 19199 2.2.3 Merge the datasets and Describe} Now we will combine both of the samples and describe them with proper tables and figures before converting the demographics into numeric variables. \\begin{scriptsize} ethnic %&gt;% arrange(desc(Participants)) %&gt;% kable(., &quot;html&quot;, booktabs = T, escape = F, caption = &quot;Race/Ethnicity Among U.S. Participants in the full sample&quot;) %&gt;% kable_styling(full_width = F) Table 2.1: Race/Ethnicity Among U.S. Participants in the full sample Ethnicity/Race Participants White/Caucasian 63763 African American 6354 Other 3201 Mexican 2249 Latino 2184 Chinese 1189 Native American 741 Philipino 649 Other Asian 597 Puerto Rican 555 Korean 528 Indian/Pakistani 498 Pacific Islander 329 Japanese 266 levs &lt;- tibble(old = c(&quot;less12yrs&quot;, &quot;Less than 12 years&quot;, &quot;HSgrad&quot;, &quot;High school graduate&quot;, &quot;SomeCollege&quot;, &quot;Some college did not graduate&quot;, &quot;CurrentInUniv&quot;, &quot;Currently attending college&quot;, &quot;CollegeDegree&quot;, &quot;College graduate&quot;, &quot;InGradOrProSchool&quot;, &quot;Graduate or professional degree&quot;, &quot;GradOrProDegree&quot;), new = c(rep(&quot;Less than 12 years&quot;, 2), rep(&quot;High school graduate&quot;, 2), rep(&quot;Some college/university, but did not graduate&quot;, 2), rep(&quot;Currently in college/university&quot;, 2), rep(&quot;College/university degree&quot;, 2), &quot;Currently in graduate or professional school&quot;, rep(&quot;Graduate or professional school degree&quot;, 2))) education %&gt;% mutate(`Educational Level` = mapvalues(`Educational Level`, levs$old, levs$new)) %&gt;% group_by(`Educational Level`) %&gt;% summarize(Participants = sum(Participants)) %&gt;% ungroup() %&gt;% mutate(`Educational Level` = factor(`Educational Level`, levels = unique(levs$new))) %&gt;% arrange(`Educational Level`) %&gt;% kable(., &quot;html&quot;, booktabs = T, escape = F, caption = &quot;Educational Attainment Level in the full sample&quot;) %&gt;% kable_styling(full_width = F) Table 2.2: Educational Attainment Level in the full sample Educational Level Participants Less than 12 years 48063 High school graduate 26554 Some college/university, but did not graduate 23742 Currently in college/university 146863 College/university degree 50316 Currently in graduate or professional school 11431 Graduate or professional school degree 37959 Itâ€™s also useful to describe the extent of the missingness in the data (for the IPIP100 variables) and the number of pairwise administrations across the items. While weâ€™re at it, weâ€™ll also bootstrap estimates of the standard errors of the item-level correlations. items &lt;- colnames(subset(fullSample, select = c(q_55:q_1989))) not_na_count &lt;- apply(fullSample[,items], 1, function(x) sum(!is.na(x))) partAdminsMean &lt;- round(mean(not_na_count), 0) partAdminsMean &lt;- prettyNum(partAdminsMean, big.mark=&quot;,&quot;, scientific=F) partAdminsMedian &lt;- round(median(not_na_count), 0) partAdminsMedian &lt;- prettyNum(partAdminsMedian, big.mark=&quot;,&quot;, scientific=F) partAdminsSD &lt;- round(sd(not_na_count), 0) partAdminsSD &lt;- prettyNum(partAdminsSD, big.mark=&quot;,&quot;, scientific=F) described &lt;- describe(fullSample[,items]) AdminsMean &lt;- round(mean(described$n), 0) AdminsMean &lt;- prettyNum(AdminsMean, big.mark=&quot;,&quot;, scientific=F) AdminsMedian &lt;- round(median(described$n), 0) AdminsMedian &lt;- prettyNum(AdminsMedian, big.mark=&quot;,&quot;, scientific=F) AdminsSD &lt;- round(sd(described$n), 0) AdminsSD &lt;- prettyNum(AdminsSD, big.mark=&quot;,&quot;, scientific=F) # The mean, median and sd of administrations of items AdminsMean AdminsMedian AdminsSD rm(described) pwiseAdmins1 &lt;- pairwiseDescribe(fullSample[,items]) pwiseAdmins &lt;- prettyNum(pwiseAdmins1, big.mark=&quot;,&quot;, scientific=F) pwiseAdminsMean &lt;- round(pwiseAdmins1$mean, 0) pwiseAdminsMean &lt;- prettyNum(pwiseAdminsMean, big.mark=&quot;,&quot;, scientific=F) pwiseAdminsMedian &lt;- round(pwiseAdmins1$median, 0) pwiseAdminsMedian &lt;- prettyNum(pwiseAdminsMedian, big.mark=&quot;,&quot;, scientific=F) pwiseAdminsSD &lt;- round(pwiseAdmins1$sd, 0) pwiseAdminsSD &lt;- prettyNum(pwiseAdminsSD, big.mark=&quot;,&quot;, scientific=F) pwiseAdminsMin &lt;- round(pwiseAdmins1$min, 0) pwiseAdminsMin &lt;- prettyNum(pwiseAdminsMin, big.mark=&quot;,&quot;, scientific=F) # The mean, median and sd of pairwise administrations of items pwiseAdminsMean pwiseAdminsMedian pwiseAdminsSD #rm(pwiseAdmins1) # the CIs around the rs IPIP100CorCI &lt;- cor.ci(fullSample[,items], p=.05, n.iter = 100, overlap=FALSE, plot = FALSE, poly = FALSE) save(IPIP100CorCI, file = &quot;~/Downloads/cor_stab.RData&quot;) load(url(sprintf(&quot;%s/results/cor_stab.RData?raw=true&quot;, data_path))) plot(density(IPIP100CorCI$sds), main=&quot;&quot;, ylab=&quot;Frequency&quot;, xlab=&quot;Standard error of item-level correlations&quot;, xlim= c(0,.02), ylim = c(0,1000)) Figure 2.1: Supplementary Figure 2. Standard error of item-level correlations Clean up some of the helper objects and save the data for further analysis later. IPIP100items04apr2006thru7feb2017 &lt;- fullSample ItemInfo100 &lt;- ItemInfo[items,1:2] keys.list.trimmed &lt;- keys.list[c(&quot;IPIP100agreeableness20&quot;,&quot;IPIP100conscientiousness20&quot;, &quot;IPIP100extraversion20&quot;, &quot;IPIP100intellect20&quot;, &quot;IPIP100EmotionalStability20&quot;)] ItemLists.trimmed &lt;- ItemLists[c(&quot;IPIP100agreeableness20&quot;,&quot;IPIP100conscientiousness20&quot;, &quot;IPIP100extraversion20&quot;, &quot;IPIP100intellect20&quot;, &quot;IPIP100EmotionalStability20&quot;)] save(IPIP100items04apr2006thru7feb2017, ItemInfo100, ItemLists.trimmed, keys.list.trimmed, file=paste(filepathdata, &quot;IPIP100items04apr2006thru7feb2017.rdata&quot;, sep=&quot;&quot;)) "],
["workspace.html", "Chapter 3 Workspace 3.1 Load and Clean Data", " Chapter 3 Workspace 3.1 Load and Clean Data First, we will load in two separate data sets from the SAPA project. The first is the larger set of items from the SAPA project, including comprehensive codebooks, while the second contains only the IPIP personality items but has more participants. Weâ€™ll use the codebooks from the former and the data from the latter. data_path &lt;- &quot;https://github.com/emoriebeck/age_nets/blob/master&quot; load(url(sprintf(&quot;%s/data/SAPAdata18aug2010thru7feb2017.rdata?raw=true&quot;, data_path))) load(url(sprintf(&quot;%s/data/IPIP100items04apr2006thru7feb2017.rdata?raw=true&quot;, data_path))) # There is one row that is duplicated twice, so we&#39;ll remove it IPIP100items04apr2006thru7feb2017 &lt;- IPIP100items04apr2006thru7feb2017 %&gt;% group_by(RID) %&gt;% filter(n() == 1) # load custom themes for qgraph source(sprintf(&quot;%s/custom_qgraph.R?raw=true&quot;, data_path)) First, we will get the item numbers for the IPIP 50. # get item #&#39;s for miniipip20 items ipip20_items &lt;- melt(ItemLists) %&gt;% filter(grepl(&quot;miniIPIP20&quot;, L1) == T &amp; L1 != &quot;miniIPIP20&quot;) %&gt;% mutate(value = as.character(value)) %&gt;% arrange(L1) %&gt;% group_by(L1) %&gt;% mutate(name = seq(1,n(),1), name = paste(L1, name, sep = &quot;_&quot;)) %&gt;% ungroup() # get item #&#39;s for ipip50 items ipip50_items &lt;- melt(ItemLists) %&gt;% filter(grepl(&quot;IPIP50&quot;, L1) == T &amp; L1 != &quot;IPIP50&quot;) %&gt;% mutate(value = as.character(value)) %&gt;% arrange(L1) ipip50_items &lt;- ipip50_items %&gt;% group_by(L1) %&gt;% mutate(name = seq(1,n(),1), name = paste(L1, name, sep = &quot;_&quot;)) %&gt;% ungroup() # get item #&#39;s for the ipip100 items ipip100_items &lt;- melt(ItemLists) %&gt;% filter(grepl(&quot;IPIP100&quot;, L1) == T &amp; L1 != &quot;IPIP100&quot;) %&gt;% mutate(value = as.character(value)) %&gt;% rbind(c(&quot;q_55&quot;, &quot;IPIP100extraversion20&quot;)) %&gt;% arrange(L1) %&gt;% group_by(L1) %&gt;% mutate(name = seq(1,n(),1), name = paste(L1, name, sep = &quot;_&quot;)) %&gt;% ungroup() Now, weâ€™ll get item info, including the text and names of the scales. ## Get item content #ipip20 ItemInfo20 &lt;- ItemInfo100 %&gt;% filter(rownames(.) %in% ipip20_items$value) %&gt;% separate(IPIP100, into = c(&quot;Inventory&quot;, &quot;Factor&quot;)) %&gt;% mutate(Factor = factor(Factor, levels = c(&quot;A&quot;, &quot;C&quot;, &quot;ES&quot;, &quot;E&quot;, &quot;I&quot;)), Factor = recode(Factor,`A` = &quot;agreeableness&quot;, `E` = &quot;extraversion&quot;, `ES` = &quot;emotionalstability&quot;,`I` = &quot;intellect&quot;, `C` = &quot;conscientiousness&quot;)) %&gt;% arrange(Factor) #ipip50 ItemInfo50 &lt;- ItemInfo100 %&gt;% filter(rownames(.) %in% ipip50_items$value) %&gt;% separate(IPIP100, into = c(&quot;Inventory&quot;, &quot;Factor&quot;)) %&gt;% mutate(Factor = factor(Factor, levels = c(&quot;A&quot;, &quot;C&quot;, &quot;ES&quot;, &quot;E&quot;, &quot;I&quot;)), Factor = recode(Factor,`A` = &quot;agreeableness&quot;, `E` = &quot;extraversion&quot;, `ES` = &quot;emotionalstability&quot;,`I` = &quot;intellect&quot;, `C` = &quot;conscientiousness&quot;)) %&gt;% arrange(Factor) #ipip100 ItemInfo100 &lt;- ItemInfo100 %&gt;% filter(rownames(.) %in% ipip100_items$value) %&gt;% separate(IPIP100, into = c(&quot;Inventory&quot;, &quot;Factor&quot;)) %&gt;% mutate(Factor = factor(Factor, levels = c(&quot;A&quot;, &quot;C&quot;, &quot;ES&quot;, &quot;E&quot;, &quot;I&quot;)), Factor = recode(Factor,`A` = &quot;agreeableness&quot;, `E` = &quot;extraversion&quot;, `ES` = &quot;emotionalstability&quot;,`I` = &quot;intellect&quot;, `C` = &quot;conscientiousness&quot;)) %&gt;% arrange(Factor) And the column names. # get column names for ipip20 items in IPIP data ipip20_cols &lt;- c(&quot;RID&quot;, &quot;age&quot;, &quot;gender&quot;, ipip20_items$value[ipip20_items$value %in% colnames(IPIP100items04apr2006thru7feb2017)]) # get column names for ipip50 items in IPIP data ipip50_cols &lt;- c(&quot;RID&quot;, &quot;age&quot;, &quot;gender&quot;, ipip50_items$value[ipip50_items$value %in% colnames(IPIP100items04apr2006thru7feb2017)]) # get column names for ipip100 items in IPIP data ipip100_cols &lt;- c(&quot;RID&quot;, &quot;age&quot;, &quot;gender&quot;, ipip100_items$value[ipip100_items$value %in% colnames(IPIP100items04apr2006thru7feb2017)]) And subset the data based on those items into new data frames. # subset IPIP50 &amp; IPIP100 SAPA data ipip20 &lt;- IPIP100items04apr2006thru7feb2017[, ipip20_cols] ipip50 &lt;- IPIP100items04apr2006thru7feb2017[, ipip50_cols] ipip100 &lt;- IPIP100items04apr2006thru7feb2017[, ipip100_cols] And then rename the column names of the new data frames using their putative Big 5 Traits. # rename columns by trait colnames(ipip20)[4:23] &lt;- ipip20_items$name colnames(ipip50)[4:53] &lt;- ipip50_items$name colnames(ipip100)[4:103] &lt;- ipip100_items$name And create a list of column names for later. e &lt;- c(&quot;quiet&quot;, &quot;life_party&quot;, &quot;draw_attention&quot;, &quot;center_attention&quot;, &quot;dont_talk&quot;, &quot;comfortable_others&quot;, &quot;little2say&quot;, &quot;background&quot;, &quot;start_convo&quot;, &quot;talk@parties&quot;) a &lt;- c(&quot;int_people&quot;, &quot;-int_problems&quot;, &quot;-int_others&quot;, &quot;-concern&quot;, &quot;others_emotions&quot;, &quot;soft_heart&quot;, &quot;insult&quot;, &quot;ease&quot;, &quot;sympathize&quot;, &quot;time4others&quot;) c &lt;- c(&quot;prepared&quot;, &quot;exacting&quot;, &quot;schedule&quot;, &quot;chores&quot;, &quot;leave_belongings&quot;, &quot;order&quot;, &quot;make_mess&quot;, &quot;forget_place&quot;, &quot;details&quot;, &quot;shirk_duties&quot;) n &lt;- c(&quot;disturbed&quot;, &quot;relaxed&quot;, &quot;change_mood&quot;, &quot;irritated&quot;, &quot;stressed&quot;, &quot;upset&quot;, &quot;mood_swings&quot;, &quot;often_blue&quot;, &quot;seldon_blue&quot;, &quot;worry&quot;) o &lt;- c(&quot;ideas&quot;, &quot;not_abstract&quot;, &quot;quick_understand&quot;, &quot;no_imagination&quot;, &quot;rich_vocab&quot;, &quot;imagination&quot;, &quot;diff_abstract&quot;, &quot;exc_ideas&quot;, &quot;reflect&quot;, &quot;diff_words&quot;) all_cols20 &lt;- paste(rep(c(&quot;a&quot;, &quot;c&quot;, &quot;n&quot;, &quot;e&quot;, &quot;o&quot;), each = 4), c(paste(&quot;0&quot;, seq(1,4,1), sep = &quot;&quot;)), sep = &quot;&quot;) all_cols50 &lt;- paste(rep(c(&quot;a&quot;, &quot;c&quot;, &quot;n&quot;, &quot;e&quot;, &quot;o&quot;), each = 10), c(paste(&quot;0&quot;, seq(1,9,1), sep = &quot;&quot;), &quot;10&quot;), sep = &quot;&quot;) all_cols100 &lt;- paste(rep(c(&quot;a&quot;, &quot;c&quot;, &quot;n&quot;, &quot;e&quot;, &quot;o&quot;), each = 20), c(paste(&quot;0&quot;, seq(1,9,1), sep = &quot;&quot;), seq(10,20,1)), sep = &quot;&quot;) Now, weâ€™ll create a function to recode the age variable to account for the smaller sample sizes among the older participants in the sample. # create new age variable since sample sizes too small for older ages # necessary to have all pairwise observations for correlations recode_age &lt;- function(df){ df$age2 &lt;- df$age df$age2[df$age &gt;= 60 &amp; df$age &lt; 65] &lt;- 62.5 df$age2[df$age &gt;= 65 &amp; df$age &lt; 70] &lt;- 67.5 df$age2[df$age &gt;= 70 &amp; df$age &lt; 75] &lt;- 72.5 df$age2[df$age &gt;= 75] &lt;- 78 # based on median of sample &gt;= 75 df$age2 &lt;- factor(df$age2) df &lt;- df %&gt;% mutate(age_groups = mapvalues(age, 10:79, rep(1:7, each = 10), warn_missing = F)) %&gt;% tbl_df } ipip20 &lt;- recode_age(ipip20) ipip50 &lt;- recode_age(ipip50) ipip100 &lt;- recode_age(ipip100) rm(list = ls()[grepl(&quot;^Item&quot;, ls()) | grepl(&quot;keys&quot;, ls())]) "],
["fit-the-multi-trait-networks-mtnetworks.html", "Chapter 4 Fit the Multi-Trait Networks {MTnetworks} 4.1 Multi-Trait Networks", " Chapter 4 Fit the Multi-Trait Networks {MTnetworks} 4.1 Multi-Trait Networks First, we have to do some prep to get the data into list form to run parLapply on the function to fit the networks. # create monochromatic purple color theme for node groups ipipcolors &lt;- RColorBrewer::brewer.pal(5,&quot;Set3&quot;) # remove unnecessary demographics from data data20 &lt;- ipip20 %&gt;% select(-RID, -gender) colnames(data20)[2:21] &lt;- all_cols20 data50 &lt;- ipip50 %&gt;% select(-RID, -gender) colnames(data50)[2:51] &lt;- c(a, c, n, e, o) data100 &lt;- ipip100 %&gt;% select(-RID, -gender) colnames(data100)[2:101] &lt;- all_cols100 Before we can run the networks, we are going to group them in two ways, once by decade and once by year. # transform data to list for use in parLapply for year by year groups datalist20 &lt;- dlply(select(data20, -age, -age_groups), .(age2)) datalist50 &lt;- dlply(select(data50, -age, -age_groups), .(age2)) datalist100 &lt;- dlply(select(data100,-age, -age_groups), .(age2)) # transform data to list for use in parLapply for decade groups datalist20gr &lt;- dlply(select(data20, -age, -age2), .(age_groups)) datalist50gr &lt;- dlply(select(data50, -age, -age2), .(age_groups)) datalist100gr &lt;- dlply(select(data100,-age, -age2), .(age_groups)) # remove the age group column datalist20gr &lt;- llply(datalist20gr, function(x) x %&gt;% select(-age_groups)) datalist50gr &lt;- llply(datalist50gr, function(x) x %&gt;% select(-age_groups)) datalist100gr &lt;- llply(datalist100gr, function(x) x %&gt;% select(-age_groups)) # remove the later decades with a small sample size datalist20gr &lt;- datalist20gr[!names(datalist20gr) %in% 80:90] datalist50gr &lt;- datalist50gr[!names(datalist50gr) %in% 80:90] datalist100gr &lt;- datalist100gr[!names(datalist100gr) %in% 80:90] # create group membership list based on item codes ipipgroup20 &lt;- list(a = seq(1,4,1), c = seq(5,8,1), n = seq(9,12,1), e = seq(13,16,1), o = seq(17,20,1)) ipipgroup50 &lt;- list(a = seq(1,10,1), c = seq(11,20,1), n = seq(21,30,1), e = seq(31,40,1), o = seq(41,50,1)) ipipgroup100 &lt;- list(a = seq(1,20,1), c = seq(21,40,1), n = seq(41,60,1), e = seq(61,80,1), o = seq(81,100,1)) Now we can run the networks. # Calculate the number of cores no_cores &lt;- detectCores() - 1 # Initiate cluster cl &lt;- makeCluster(no_cores) # import global env variables for parallel computing clusterExport(cl, varlist = c(&quot;ipipgroup20&quot;, &quot;ipipgroup50&quot;, &quot;ipipgroup100&quot;, &quot;ipipcolors&quot;, &quot;datalist50&quot;, &quot;ItemInfo50&quot;, &quot;ItemInfo100&quot;, &quot;ItemInfo20&quot;)) # # calculate pairwise cors # run the by year networks allcorsNgraphs20 &lt;- parLapply(cl, datalist20, EDBqgraph20n) allcorsNgraphs50 &lt;- parLapply(cl, datalist50, EDBqgraph50n) allcorsNgraphs100 &lt;- parLapply(cl, datalist100, EDBqgraph100n) # run the by decade networks allcorsNgraphs20gr &lt;- parLapply(cl, datalist20gr, EDBqgraph20n) allcorsNgraphs50gr &lt;- parLapply(cl, datalist50gr, EDBqgraph50n) allcorsNgraphs100gr &lt;- parLapply(cl, datalist100gr, EDBqgraph100n) # run centrality on the networks allcentrality20 &lt;- parLapply(cl, llply(allcorsNgraphs20, `[[`,2), function(x) qgraph::centrality_auto(x)) allcentrality50 &lt;- parLapply(cl, llply(allcorsNgraphs50, `[[`,2), function(x) qgraph::centrality_auto(x)) allcentrality100 &lt;- parLapply(cl, llply(allcorsNgraphs100, `[[`,2), function(x) qgraph::centrality_auto(x)) stopCluster(cl) # end parallel computing session # save(allcorsNgraphs20, allcorsNgraphs50, allcorsNgraphs100, # allcorsNgraphs20gr, allcorsNgraphs50gr, allcorsNgraphs100gr, # allcentrality20, allcentrality50, allcentrality100, # file = &quot;~/Box/networks/SAPA/allcors.RData&quot;) Now, to avoid redundancy, we will make create a tibble with the networks and centrality stored as list columns, which will make it easier to run functions on them efficiently. MT_net_nested &lt;- tibble(inventory = &quot;IPIP20&quot;, age = names(allcorsNgraphs20), results = allcorsNgraphs20, centrality = allcentrality20) %&gt;% mutate(cols = lapply(1:nrow(.), function(x)all_cols20)) %&gt;% bind_rows(tibble(inventory = &quot;IPIP50&quot;, age = names(allcorsNgraphs50), results = allcorsNgraphs50, centrality = allcentrality50) %&gt;% mutate(cols = lapply(1:nrow(.), function(x)all_cols50))) %&gt;% bind_rows(tibble(inventory = &quot;IPIP100&quot;, age = names(allcorsNgraphs100), results = allcorsNgraphs100, centrality = allcentrality100) %&gt;% mutate(cols = lapply(1:nrow(.), function(x)all_cols100))) %&gt;% mutate(mat = map(results, ~.[[1]]), net = map(results, ~.[[2]])) MT_net_nested_gr &lt;- tibble(age_group = names(allcorsNgraphs20gr), results = allcorsNgraphs20gr, cols = list(all_cols20), inventory = &quot;IPIP20&quot;) %&gt;% bind_rows(tibble(age_group = names(allcorsNgraphs50gr), results = allcorsNgraphs50gr, cols = list(all_cols50), inventory = &quot;IPIP50&quot;)) %&gt;% bind_rows(tibble(age_group = names(allcorsNgraphs100gr), results = allcorsNgraphs100gr, cols = list(all_cols100), inventory = &quot;IPIP100&quot;)) %&gt;% mutate(mat = map(results, function(x) x[[1]]), net = map(results, function(x) x[[2]])) # run correlations and networks for 20&#39;s corsNgraphsgr &lt;- data50 %&gt;% filter(as.numeric(age) &lt; 80) %&gt;% select(-age2, -age) %&gt;% group_by(age_groups) %&gt;% nest() %&gt;% mutate(net = map(data, ~EDBqgraphAvLayout(., ItemInfo50, ipipgroup50))) %&gt;% mutate(cols = lapply(1:nrow(.), function(x)all_cols50)) # save(MT_net_nested, MT_net_nested_gr, file = sprintf(&quot;%s/results/mt_nested_nets.RData&quot;, data_path)) load(url(sprintf(&quot;%s/results/mt_nested_nets.RData?raw=true&quot;, data_path))) getEdges.df &lt;- function(x, cols) { y &lt;- qgraph::getWmat(x) y[upper.tri(y, diag = T)] &lt;- NA colnames(y) &lt;- cols; rownames(y) &lt;- cols nvar &lt;- dim(y)[2] df &lt;- tbl_df(y) %&gt;% mutate(from = row.names(y)) %&gt;% gather(key = to, value = weight, 1:nvar) %&gt;% mutate(edge = paste(from, to, sep = &quot;_&quot;)) %&gt;% filter(!is.na(weight)) return(df) } MT_net_nested &lt;- MT_net_nested %&gt;% mutate(edges.df = map2(net, cols, getEdges.df), cors.df = map2(mat, cols, getEdges.df)) MT_net_nested_gr &lt;- MT_net_nested_gr %&gt;% mutate(edges.df = map2(net, cols, getEdges.df), cors.df = map2(mat, cols, getEdges.df)) ex_fun &lt;- function(x){x[[1]]} corsNgraphsgr &lt;- corsNgraphsgr %&gt;% mutate(cols = lapply(1:nrow(.), function(x)all_cols50), mat = map(net, function(x) x[[1]]), net = map(net, function(x) x[[2]]), edges.df = map2(net, cols, getEdges.df), cors.df = map2(mat, cols, getEdges.df)) "],
["STnetworks.html", "Chapter 5 Fit the Single Trait Networks", " Chapter 5 Fit the Single Trait Networks Now, we will fit networks for each trait separately, which means they will not account for relationships between traits to which the indicators putatively belong. ST_net_nested &lt;- ipip20 %&gt;% mutate(inventory = &quot;IPIP20&quot;) %&gt;% setNames(c(&quot;RID&quot;, &quot;age&quot;, &quot;gender&quot;, all_cols20, &quot;age2&quot;, &quot;inventory&quot;)) %&gt;% gather(key = item, value = value, -RID, -age, -gender, -inventory, na.rm = T) %&gt;% full_join( ipip50 %&gt;% mutate(inventory = &quot;IPIP50&quot;) %&gt;% setNames(c(&quot;RID&quot;, &quot;age&quot;, &quot;gender&quot;, all_cols50, &quot;age2&quot;, &quot;inventory&quot;)) %&gt;% gather(key = item, value = value, -RID, -age, -gender, -inventory, na.rm = T) ) %&gt;% full_join( ipip100 %&gt;% mutate(inventory = &quot;IPIP100&quot;) %&gt;% setNames(c(&quot;RID&quot;, &quot;age&quot;, &quot;gender&quot;, all_cols100, &quot;age2&quot;, &quot;inventory&quot;)) %&gt;% gather(key = item, value = value, -RID, -age, -gender, -inventory, na.rm = T) ) %&gt;% tbl_df %&gt;% separate(item, c(&quot;trait&quot;, &quot;number&quot;), 1, remove = F) %&gt;% select(inventory, RID, age, trait, item, value) %&gt;% group_by(inventory, age, trait) %&gt;% nest() %&gt;% mutate(data = map(data, function(x){x %&gt;% spread(key = item, value = value)}), cols = map(data, colnames), results = map(data, EDBqgraph2), mat = map(results, ~.[[1]]), net = map(results, ~.[[2]]), centrality = map(net, centrality_auto)) # save(ST_net_nested, file = sprintf(&quot;%s/results/st_nested_nets.RData&quot;, data_path)) load(url(sprintf(&quot;%s/results/st_nested_nets.RData?raw=true&quot;, data_path))) rm(list = ls()[(grepl(&quot;data&quot;, ls()) | grepl(&quot;ipip&quot;, tolower(ls()))) &amp; ! grepl(&quot;colors&quot;, ls())]) ST_net_nested &lt;- ST_net_nested %&gt;% mutate(edges.df = map2(net, cols, getEdges.df), cors.df = map2(mat, cols, getEdges.df)) "],
["pcors.html", "Chapter 6 Age Differences in Coherence and Differentiation 6.1 Multi-Trait Coherence and Differentiation 6.2 Single and Mutli-Trait Coherence", " Chapter 6 Age Differences in Coherence and Differentiation trait.edges.sum &lt;- ST_net_nested %&gt;% unnest(edges.df) %&gt;% group_by(inventory, age, Trait) %&gt;% summarize(mean = fisherz2r(mean(fisherz(abs(weight)), na.rm = T))) %&gt;% ungroup() %&gt;% group_by(age) %&gt;% mutate(type = &quot;Single Trait&quot;, comp = &quot;ST-Coherence&quot;, gmean = fisherz2r(mean(fisherz(mean), na.rm = T)), traits = Trait) trait.edges.sum50 &lt;- trait.edges.sum %&gt;% filter(inventory == &quot;IPIP50&quot;) edges.sum &lt;- MT_net_nested %&gt;% unnest(edges.df) %&gt;% separate(from, c(&quot;from_trait&quot;, &quot;from_item&quot;), 1) %&gt;% separate(to, c(&quot;to_trait&quot;, &quot;to_item&quot;), 1) %&gt;% unite(traits, from_trait, to_trait, remove = F) %&gt;% mutate(comp = ifelse(from_trait == to_trait, &quot;Coherence&quot;, &quot;Differentiation&quot;)) %&gt;% group_by(inventory, age, traits, comp, from_trait, to_trait) %&gt;% summarize(n = n(), mean = fisherz2r(mean(fisherz(abs(weight)), na.rm = T))) %&gt;% ungroup() %&gt;% group_by(inventory, age, from_trait, comp) %&gt;% mutate(traitMean1 = fisherz2r(mean(fisherz(mean), na.rm = T))) %&gt;% ungroup() %&gt;% group_by(inventory, age, to_trait, comp) %&gt;% mutate(traitMean2 = fisherz2r(mean(fisherz(mean), na.rm = T))) %&gt;% ungroup() %&gt;% mutate(traitMean = fisherz2r(rowMeans(cbind(fisherz(traitMean1), fisherz(traitMean2)), na.rm = T))) %&gt;% ungroup() %&gt;% group_by(inventory, age, comp) %&gt;% mutate(type = &quot;Multi Trait&quot;, gmean = fisherz2r(mean(fisherz(mean), na.rm = T))) %&gt;% arrange(age, comp, from_trait) %&gt;% mutate(Trait = ifelse(comp == &quot;Coherence&quot;, toupper(from_trait), NA)) edges.sum$traitMean &lt;- with(edges.sum, rowMeans(cbind(traitMean1, traitMean2), na.rm = T)) 6.1 Multi-Trait Coherence and Differentiation edges.sum %&gt;% full_join(trait.edges.sum) %&gt;% filter(inventory == &quot;IPIP50&quot;) %&gt;% ggplot(aes(x = as.numeric(age), y = mean)) + geom_smooth(data = edges.sum %&gt;% filter(inventory == &quot;IPIP50&quot;), span = .3, aes(group = traits), alpha = .1, color = &quot;gray&quot;, size = .2, se = F) + geom_smooth(data = edges.sum %&gt;% filter(inventory == &quot;IPIP50&quot;), size = 1, aes(y = gmean, group = comp, color = comp), se = F, span = .3) + # geom_line(data = trait.edges.sum %&gt;% filter(inventory == &quot;IPIP50&quot;), # aes(group = traits), alpha = .1) + # geom_line(data = trait.edges.sum %&gt;% filter(inventory == &quot;IPIP50&quot;), size = 1, # aes(y = gmean, group = comp, color = comp)) + scale_x_continuous(limits = c(14,80), breaks = seq(15,80,5)) + scale_y_continuous(limits = c(0,.2), breaks = seq(0,.2, .05)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;)) + scale_color_manual(values = c(&quot;royalblue1&quot;, &quot;mediumseagreen&quot;)) + labs(x = &quot;Age&quot;, y = &quot;Average Edge Weight&quot;, color = NULL) + guides(linetype = F) + theme_classic() + theme(axis.text.x = element_text(size = rel(.75), face = &quot;bold&quot;, angle = 90), axis.text.y = element_text(face = &quot;bold&quot;), legend.key.size = unit(.4, &quot;cm&quot;), legend.text = element_text(size = rel(.7)), legend.title = element_text(size = rel(.7), face = &quot;bold&quot;), legend.position = c(.15, .9)) ## Joining, by = c(&quot;inventory&quot;, &quot;age&quot;, &quot;traits&quot;, &quot;comp&quot;, &quot;mean&quot;, &quot;type&quot;, &quot;gmean&quot;, &quot;Trait&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; (#fig:diff coh plots)Figure 1. Coherence (green) and differentiation (blue) across the lifespan. Coherence is indicated by higher average inter-item partial correlations within scales, while differentiation is indicated by lower average-inter-item partial correlations across sub-scales of the Big Five. Thus, increases in the blue differentiation line indicate decreases in differentiation and vice versa. Light gray lines show the unique trajectory for each of the Big Five. # ggsave(sprintf(&quot;%s/photos/differentiation_coherence_line_graph.png&quot;, data_path), # width = 5, height = 4) 6.2 Single and Mutli-Trait Coherence edges.sum %&gt;% full_join(trait.edges.sum) %&gt;% filter(comp != &quot;Differentiation&quot; &amp; inventory == &quot;IPIP50&quot;) %&gt;% ggplot(aes(x = as.numeric(age), y = mean, group = comp)) + geom_line(data = filter(edges.sum, comp == &quot;Coherence&quot; &amp; inventory == &quot;IPIP50&quot;), aes(color = Trait, linetype = type)) + geom_line(data = trait.edges.sum %&gt;% filter(inventory == &quot;IPIP50&quot;), aes(color = Trait, linetype = type)) + scale_x_continuous(limits = c(14,80), breaks = seq(15, 80,5)) + scale_linetype_manual(values = c(&quot;dotted&quot;, &quot;solid&quot;)) + guides(color = F) + facet_wrap(~Trait, nrow = 3) + labs(x = &quot;Age&quot;, y = &quot;Average Edge Weight&quot;, color = &quot;Big5\\nTrait&quot;, linetype = &quot;Network\\nType&quot;) + theme_classic() + theme(axis.text.x = element_text(size = rel(.5), face = &quot;bold&quot;, angle = 90), axis.text.y = element_text(face = &quot;bold&quot;), legend.position = c(.75,.15), legend.key.size = unit(.5, &quot;cm&quot;), legend.text = element_text(size = rel(.75)), legend.title = element_text(size = rel(.75), face = &quot;bold&quot;)) ## Joining, by = c(&quot;inventory&quot;, &quot;age&quot;, &quot;traits&quot;, &quot;comp&quot;, &quot;mean&quot;, &quot;type&quot;, &quot;gmean&quot;, &quot;Trait&quot;) (#fig:st coherence plot)Figure 2. Coherence across the lifespan by trait. Solid lines indicate coherence when each trait was modeled separately, while dotted lines indicate coherence when the full IPIP-50 Big Five scale was modeled simultaneously. Coherence is indicated by higher average inter-item partial correlations within scales; thus increases in coherence across age groups suggests increasing coherence. # ggsave(sprintf(&quot;%s/photos/by_trait_differentiation_coherence_line_graph.png&quot;, data_path), # width = 5, height = 4) "],
["communities.html", "Chapter 7 Community Structure", " Chapter 7 Community Structure Next, we calculate the communities in each network using the Spin Glass algorithm. Although there is no widely accepted definition of communities (Baroncelli, 2012), communities are generally considered to be nodes that are more connected to each other than to other nodes. For demonstrative purposes, we separately collapse across the 20s and 60s and run the networks. Then we calculate their community structure and compare them. ## Multi-Trait ### Run Communities communities_fun &lt;- function(g, cols){ g &lt;- igraph::as.igraph(g) V(g)$label &lt;- cols g2 &lt;- g E(g2)$weight &lt;- abs(E(g2)$weight) lvc &lt;- igraph::cluster_louvain(g2) subgraphs &lt;- list() for (i in 1:length(unique(lvc$membership))){ subgraphs[[i]] &lt;- igraph::induced_subgraph(g, vids = which(lvc$membership == i), impl = &quot;copy_and_delete&quot;)} results &lt;- list(graph = g, louvain = lvc, subgraphs = subgraphs) return(results) } membership_fun &lt;- function(comm, cols){ comm &lt;- comm$louvain$membership %&gt;% setNames(cols) } MT_net_nested &lt;- MT_net_nested %&gt;% mutate(communities = map2(net, cols, communities_fun), membership = map2(communities, cols, possibly(membership_fun, NA_real_))) MT_net_nested_gr &lt;- MT_net_nested_gr %&gt;% mutate(communities = map2(net, cols, communities_fun), membership = map2(communities, cols, membership_fun)) communitiesall &lt;- (MT_net_nested %&gt;% filter(inventory == &quot;IPIP50&quot;))$membership %&gt;% ldply(.) communities38below &lt;- (MT_net_nested %&gt;% filter(inventory == &quot;IPIP50&quot; &amp; age &lt;= 38))$membership %&gt;% ldply(.) communities39above &lt;- (MT_net_nested %&gt;% filter(inventory == &quot;IPIP50&quot; &amp; age &gt;= 39))$membership %&gt;% ldply(.) 7.0.1 Plots communities20s &lt;- (MT_net_nested_gr %&gt;% filter(inventory == &quot;IPIP50&quot; &amp; age_group == 2))$communities[[1]] communities60s &lt;- (MT_net_nested_gr %&gt;% filter(inventory == &quot;IPIP50&quot; &amp; age_group == 6))$communities[[1]] b_color_fun &lt;- function(c){ mapvalues(c$louvain$membership, from = seq(1, max(c$louvain$membership),1), to = RColorBrewer::brewer.pal(max(c$louvain$membership),&quot;Set3&quot;)) } MT_net_nested_gr &lt;- MT_net_nested_gr %&gt;% mutate(bordercolors = map(communities, b_color_fun), net = map2(net, bordercolors, EDBqgraph_communities)) %&gt;% arrange(age_group) # pdf(filename = sprintf(&quot;%s/SAPA/photos/nets_20s_60s.bmp&quot;, data_path), width = 800, height = 400) par(mfrow = c(1,2)) plot((MT_net_nested_gr %&gt;% filter(inventory == &quot;IPIP50&quot; &amp; age_group == 2))$net[[1]]) title(&quot;20s&quot;) plot((MT_net_nested_gr %&gt;% filter(inventory == &quot;IPIP50&quot; &amp; age_group == 6))$net[[1]]) title(&quot;60s&quot;) (#fig:comm plots)Figure 3. Item networks by age group. Nodes (circles) represent unique IPIP-50 items. Nodes colors are colored according to their putative Big Five scale membership, while node borders are colored according to their empirically derived community membership. Edges (lines between nodes) indicate partial correlations. Solid lines indicate positive relationships, while dashed lines indicate negative relationships. # dev.off() par(mfrow = c(2,4)) lapply(1:7, function(x){ plot((MT_net_nested_gr %&gt;% filter(inventory == &quot;IPIP50&quot;))$net[[x]]); title(sprintf(&quot;%s0s&quot;, x))}) ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL (#fig:age plots)Supplementary Figure 3. Network Structure by Decade 7.0.2 Matching Across Communities The code below is a little incomprehensible, but in this instance there isnâ€™t really a better alternative. match_mat &lt;- matrix(rep(NA, 50*50), nrow = 50) for(i in 1:50){ # for(j in 1:50){ match &lt;- 0L for(k in 1:50){ match &lt;- ifelse(communitiesall[k,i] == communitiesall[k,j], match + 1, match) } match_mat[i,j] &lt;- match } } match_mat38below &lt;- matrix(rep(NA, 50*50), nrow = 50) match_mat39above &lt;- matrix(rep(NA, 50*50), nrow = 50) for(i in 1:50){ for(j in 1:50){ match38below &lt;- 0L match39above &lt;- 0L for(k in 1:25){ match38below &lt;- ifelse(communities38below[k,i] == communities38below[k,j], match38below + 1, match38below) match39above &lt;- ifelse(communities39above[k,i] == communities39above[k,j], match39above + 1, match39above) } match_mat38below[i,j] &lt;- match38below match_mat39above[i,j] &lt;- match39above } } community_plot_fun &lt;- function(df, age, lim){ colnames(df) &lt;- all_cols50; rownames(df) &lt;- all_cols50 df[upper.tri(df, diag = T)] &lt;- NA tbl_df(df) %&gt;% mutate(Edge1 = factor(all_cols50, levels = all_cols50)) %&gt;% gather(key = Edge2, value = communities, 1:50) %&gt;% mutate(Edge2 = factor(Edge2, levels = all_cols50)) %&gt;% filter(!is.na(communities)) %&gt;% ggplot(aes(x = Edge2, y = Edge1, fill = communities)) + geom_raster() + scale_fill_gradient( high = &quot;orchid4&quot;, low = &quot;white&quot;, limit = c(0,lim), name=&quot;Shared\\nCommunity\\nSums&quot;) + labs(x = NULL, y = NULL, title = sprintf(&quot;%s&quot;, age)) + theme_classic() + theme(axis.text.x = element_text(face = &quot;bold&quot;, angle = 90), axis.text.y = element_text(face = &quot;bold&quot;), plot.title = element_text(hjust = .5, face = &quot;bold&quot;)) } communityall &lt;- community_plot_fun(match_mat, &quot;All&quot;, 50) + theme(legend.position = c(.9, .5)) community38below &lt;- community_plot_fun(match_mat38below, &quot;38 &amp; below&quot;, 25) + theme(legend.position = &quot;none&quot;, axis.text = element_text(size = rel(.3))) community39above &lt;- community_plot_fun(match_mat39above, &quot;39 &amp; above&quot;, 25) + theme(legend.position = &quot;none&quot;, axis.text = element_text(size = rel(.3))) community_plot &lt;- grid.arrange(communityall, community38below, community39above, layout_matrix = rbind(c(1,1,3), c(1,1,4))) (#fig:comm plot)Figure 4. Frequency of shared community membership across items in (A) the full sample (B), participants 38 and below, (C) and participants 39 and above. Groups were split according to a median age split of all age groups for ease of comparing frequency, The diagonal of each plot indicates items within the same Big 5 scale, while off-diagonal elements are largely items of different scales. Darker colors indicate that the empirically derived community membership of two nodes was frequently shared across age groups, while lighter colors indicate that community membership was often not shared. # ggsave(plot = community_plot, file = sprintf(&quot;%s/photos/shared_communities.png&quot;, data_path), # width = 9, height = 6) "],
["procors.html", "Chapter 8 Profile Correlations 8.1 Profiles Correlation Plots (Figure 5)", " Chapter 8 Profile Correlations Next, we calculate profile correlations to test the stability of edges across all possible pairwise combinations of ages. A strong parallel correlation indicates that the weight of the edges is associated across two ages. We then map these profile correlations onto a heat map that displays the pairwise profile correlations. We would expect to see the strongest profile correlations cluster on the diagonal of heatmap â€“ that is between ages that are adjacent or near in age. ## Multi-Trait wide_fun &lt;- function(df){ df &lt;- unclass(df) %&gt;% data.frame %&gt;% select(-from, -to) %&gt;% spread(key = edge, value = weight) rownames(df) &lt;- df$age df &lt;- df %&gt;% select(-age) return(df) } # change the direction of the data MT_procor &lt;- MT_net_nested %&gt;% unnest(edges.df) %&gt;% group_by(inventory) %&gt;% nest() %&gt;% mutate(wide = map(data, wide_fun), procor = map(wide, ~cor(t(.), use = &quot;pairwise.complete.obs&quot;))) # separate out differentiation from coherence edges.mat50bw &lt;- MT_net_nested %&gt;% filter(inventory == &quot;IPIP50&quot;) %&gt;% unnest(edges.df) %&gt;% separate(from, into = c(&quot;from_factor&quot;, &quot;from_item&quot;), 1) %&gt;% separate(to, into = c(&quot;to_factor&quot;, &quot;to_item&quot;), 1) %&gt;% filter(from_factor != to_factor) %&gt;% select(age, weight, edge) %&gt;% spread(key = edge, value = weight) %&gt;% unclass %&gt;% data.frame # separate out coherence from differentiation edges.mat50wi &lt;- MT_net_nested %&gt;% filter(inventory == &quot;IPIP50&quot;) %&gt;% unnest(edges.df) %&gt;% separate(from, into = c(&quot;from_factor&quot;, &quot;from_item&quot;), 1) %&gt;% separate(to, into = c(&quot;to_factor&quot;, &quot;to_item&quot;), 1) %&gt;% filter(from_factor == to_factor) %&gt;% select(age, weight, edge) %&gt;% spread(key = edge, value = weight) %&gt;% unclass %&gt;% data.frame # set the rownsames of these to age rownames(edges.mat50bw) &lt;- edges.mat50bw$age rownames(edges.mat50wi) &lt;- edges.mat50wi$age ############################################## ############ Profile Correlations ############ ############################################## # calculate profile correlations procor50bw &lt;- cor(t(edges.mat50bw[,-1]), use = &quot;pairwise.complete.obs&quot;) procor50wi &lt;- cor(t(edges.mat50wi[,-1]), use = &quot;pairwise.complete.obs&quot;) 8.1 Profiles Correlation Plots (Figure 5) ############################################## ################## PLOTS ##################### ############################################## procor_plot_fun &lt;- function(df, inv, leg){ df[upper.tri(df, diag = T)] &lt;- NA p &lt;- tbl_df(df) %&gt;% mutate(age1 = colnames(.)) %&gt;% gather(key = age2, value = r, 1:(ncol(.) - 1)) %&gt;% filter(!is.na(r)) %&gt;% ggplot(aes(x = age2, y = age1, fill = r)) + geom_raster(aes(fill = r)) + scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = .5, limit = c(0,1), space = &quot;Lab&quot;, name=&quot;Profile\\nCorrelation&quot;) + labs(x = &quot;Age&quot;, y = &quot;Age&quot;, title = sprintf(&quot;Profile Correlations for %s&quot;, inv)) + theme_classic() + theme(legend.position = leg, axis.text = element_text(face = &quot;bold&quot;), axis.text.x = element_text(angle = 90, size = rel(.8)), plot.title = element_text(hjust = .5)) # ggsave(p, file = sprintf(&quot;%s/photos/Big5_%s_profile_cors_heatmap.png&quot;, data_path, inv), width = 12, height = 8) p } MT_procor &lt;- MT_procor %&gt;% mutate(plot = map2(procor, inventory, ~procor_plot_fun(.x, .y, leg = &quot;none&quot;))) layout &lt;- rbind(c(1, 1, 2), c(1, 1, 3)) p &lt;- gridExtra::grid.arrange( MT_procor$plot[[2]], procor_plot_fun(procor50bw, &quot;IPIP50 Cross-Trait Edges&quot;, leg = &quot;none&quot;), procor_plot_fun(procor50wi, &quot;IPIP50 Within-Trait Edges&quot;, leg = &quot;none&quot;), layout_matrix = layout) # ggsave(p, file = sprintf(&quot;%s/photos/Big5_IPIP50_profile_cors_heatmap.png&quot;, data_path), # width = 12, height = 8) rm(list = ls()) "]
]
